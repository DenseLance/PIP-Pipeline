{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0031cc13-5ba0-4260-856e-8a4cd9e045c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pyiqa\n",
    "import logging\n",
    "import datasets\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"MODELSCOPE_LOG_LEVEL\"] = str(logging.ERROR)\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "seed = 42\n",
    "images_evaluated_per_prompt = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ce176-8bc8-472f-a1ed-87279e795731",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3790330-5fe6-4908-a49e-7890b20e53f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"cifar/original_prompt/\"\n",
    "image_gen_dir = \"generated_images/\" + result_dir\n",
    "eval_dir = \"eval/\" + result_dir\n",
    "\n",
    "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"] # CIFAR10\n",
    "prompts = [f\"a photo of a {cls}\" for cls in classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05933ad9-dbe4-4130-a919-f6ad534eeae0",
   "metadata": {},
   "source": [
    "## Generate Images for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e791e2b-d503-409d-87b4-95a5375bc5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    safety_checker = None\n",
    ").to(device)\n",
    "pipeline.set_progress_bar_config(disable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a4c78b-cbf8-4e26-b4fa-d5ad356a02c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(image_gen_dir, exist_ok = True)\n",
    "os.makedirs(eval_dir, exist_ok = True)\n",
    "for prompt in tqdm(prompts):\n",
    "    images = pipeline(prompt.strip(), num_images_per_prompt = images_evaluated_per_prompt, generator = torch.manual_seed(seed)).images\n",
    "    for i, image in enumerate(images):\n",
    "        image.save(image_gen_dir + f\"{prompt.split(' ')[-1]}_{i}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600260c5-0879-4830-a25b-8ce61555bcea",
   "metadata": {},
   "source": [
    "## Alignment Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379c41a7-aa2b-45e4-86cc-cfa3c752fca4",
   "metadata": {},
   "source": [
    "### DSG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fc62ac-0fce-489b-a04d-74aab8132cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DSG.dsg.query_utils import generate_dsg\n",
    "from DSG.dsg.vqa_utils import MPLUG, calc_vqa_score\n",
    "from DSG.dsg.parse_utils import parse_question_output\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "vqa_model = MPLUG()\n",
    "vqa_model.pipeline_vqa.use_reentrant = False\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B\", device_map = device, torch_dtype = torch.bfloat16)\n",
    "llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm.generation_config.pad_token_id = llm_tokenizer.pad_token_id\n",
    "\n",
    "def autocomplete(prompt, max_new_tokens = 256, **kwargs):\n",
    "    inputs = llm_tokenizer([prompt], return_tensors = \"pt\", padding = True).to(device)\n",
    "    output_ids = llm.generate(**inputs, generation_config = llm.generation_config, max_new_tokens = max_new_tokens, **kwargs)\n",
    "    return llm_tokenizer.batch_decode(output_ids[:, inputs.input_ids.size(dim = 1):])[0].rstrip(llm_tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8128bbef-dac0-417e-a4e0-017b75208378",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2prompts = {i: {\"input\": prompt} for i, prompt in enumerate(prompts)}\n",
    "\n",
    "_, id2question_outputs, _ = generate_dsg(id2prompts, generate_fn = autocomplete, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa725ea-e6d7-43f7-9ab5-4eebf238aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\"data\": []}\n",
    "for i, prompt in enumerate(tqdm(prompts)):\n",
    "    images = [Image.open(image_gen_dir + f\"{prompt.split(' ')[-1]}_{j}.png\") for j in range(images_evaluated_per_prompt)]\n",
    "    qid2question = parse_question_output(id2question_outputs[i][\"output\"])\n",
    "    qid2answers = [{qid: vqa_model.vqa(image, question).lower() for qid, question in qid2question.items()} for image in images]\n",
    "    dsg_scores = [calc_vqa_score(qid2answer)[\"average_score_without_dependency\"] for qid2answer in qid2answers]\n",
    "    result[\"data\"].append({\"Prompt\": prompt, \"VQA\": {\"Question\": qid2question, **{f\"Answer {j}\": qid2answer for j, qid2answer in enumerate(qid2answers)}}, \"Score\": sum(dsg_scores) / len(dsg_scores)})\n",
    "    \n",
    "with open(\"eval/cifar_prompts_dsg.json\", \"w\") as f:\n",
    "    f.write(json.dumps(result))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf45a2-abf5-4d8b-97d0-6bf794a4af7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsg_eval_dataset = datasets.load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"eval\": \"eval/cifar_prompts_dsg.json\"},\n",
    "    split = \"eval\",\n",
    "    field = \"data\"\n",
    ")\n",
    "\n",
    "print(\"Sample Eval -\", dsg_eval_dataset[0])\n",
    "print(\"DSG -\", sum(dsg_eval_dataset[\"Score\"]) / len(dsg_eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e0badd-20f7-4390-b9d1-6b424a09a300",
   "metadata": {},
   "source": [
    "### VQAScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e042cf-c1f4-49d0-8980-ddce984c29dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from t2v_metrics.t2v_metrics import VQAScore\n",
    "\n",
    "clip_flant5_score = VQAScore(model = \"clip-flant5-xl\")\n",
    "\n",
    "result = {\"data\": []}\n",
    "for prompt in tqdm(prompts):\n",
    "    result[\"data\"].append({\"Prompt\": prompt, \"Score\": clip_flant5_score(images = [image_gen_dir + f\"{prompt.split(' ')[-1]}_{i}.png\" for i in range(images_evaluated_per_prompt)], texts = [prompt]).detach().cpu().mean().item()})\n",
    "\n",
    "with open(eval_dir + \"vqascore.json\", \"w\") as f:\n",
    "    f.write(json.dumps(result))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b03eff-3e8e-40d1-9298-e9ac2b668d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqascore_eval_dataset = datasets.load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"eval\": eval_dir + \"vqascore.json\"},\n",
    "    split = \"eval\",\n",
    "    field = \"data\"\n",
    ")\n",
    "\n",
    "print(\"Sample Eval -\", vqascore_eval_dataset[0])\n",
    "print(\"VQAScore -\", sum(vqascore_eval_dataset[\"Score\"]) / len(vqascore_eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396755a8-e960-4f20-a88e-6df089d78248",
   "metadata": {},
   "source": [
    "### CLIPScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca83556-8e97-4f4f-a575-5498d9df7e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchmetrics.multimodal.clip_score import CLIPScore\n",
    "\n",
    "clip_score = CLIPScore(model_name_or_path = \"openai/clip-vit-large-patch14\").to(device)\n",
    "\n",
    "result = {\"data\": []}\n",
    "for prompt in tqdm(prompts):\n",
    "    images = [Image.open(image_gen_dir + f\"{prompt.split(' ')[-1]}_{i}.png\") for i in range(images_evaluated_per_prompt)]\n",
    "    clip_scores = [clip_score(torch.tensor(np.asarray(image)).permute(2, 0, 1).to(device), prompt).detach().cpu().item() for image in images]\n",
    "    result[\"data\"].append({\"Prompt\": prompt, \"Score\": sum(clip_scores) / len(clip_scores)})\n",
    "    \n",
    "with open(eval_dir + \"clipscore.json\", \"w\") as f:\n",
    "    f.write(json.dumps(result))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804127c9-c1db-4bae-ba9b-22f83f4d6552",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_eval_dataset = datasets.load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"eval\": eval_dir + \"clipscore.json\"},\n",
    "    split = \"eval\",\n",
    "    field = \"data\"\n",
    ")\n",
    "\n",
    "print(\"Sample Eval -\", clip_eval_dataset[0])\n",
    "print(\"CLIPScore -\", sum(clip_eval_dataset[\"Score\"]) / len(clip_eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896a2692-c48e-4fec-9572-19e48efa0610",
   "metadata": {},
   "source": [
    "## Quality Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7fc65a-cd1b-478f-aa1d-5a6254abfab7",
   "metadata": {},
   "source": [
    "### LIQE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3294c5d-52b8-4913-8ef0-18625a5b28ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "liqe = pyiqa.create_metric(\"liqe\", device = device)\n",
    "\n",
    "result = {\"data\": []}\n",
    "for prompt in tqdm(prompts):\n",
    "    images = [Image.open(image_gen_dir + f\"{prompt.split(' ')[-1]}_{i}.png\") for i in range(images_evaluated_per_prompt)]\n",
    "    images = torch.tensor(np.stack(images)).permute(0, 3, 1, 2).to(device) / 255\n",
    "    liqe_score = liqe(images).detach().cpu().mean().item()\n",
    "    result[\"data\"].append({\"Prompt\": prompt, \"Score\": liqe_score})\n",
    "    \n",
    "with open(eval_dir + \"liqe.json\", \"w\") as f:\n",
    "    f.write(json.dumps(result))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0503ad-cd28-4d29-8d96-4c103cf6f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "liqe_eval_dataset = datasets.load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"eval\": eval_dir + \"liqe.json\"},\n",
    "    split = \"eval\",\n",
    "    field = \"data\"\n",
    ")\n",
    "\n",
    "print(\"Sample Eval -\", liqe_eval_dataset[0])\n",
    "print(\"LIQE -\", sum(liqe_eval_dataset[\"Score\"]) / len(liqe_eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850ec8f7-18f9-4f00-8b3e-5f9eba9b4f4a",
   "metadata": {},
   "source": [
    "### MUSIQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef915a29-a609-41b6-9bb1-358541c9ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "musiq = pyiqa.create_metric(\"musiq\", device = device)\n",
    "\n",
    "result = {\"data\": []}\n",
    "for prompt in tqdm(prompts):\n",
    "    images = [Image.open(image_gen_dir + f\"{prompt.split(' ')[-1]}_{i}.png\") for i in range(images_evaluated_per_prompt)]\n",
    "    images = torch.tensor(np.stack(images)).permute(0, 3, 1, 2).to(device) / 255\n",
    "    musiq_score = musiq(images).detach().cpu().mean().item()\n",
    "    result[\"data\"].append({\"Prompt\": prompt, \"Score\": musiq_score})\n",
    "    \n",
    "with open(eval_dir + \"musiq.json\", \"w\") as f:\n",
    "    f.write(json.dumps(result))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a7ebdf-301f-4a08-abd4-7fecf39351d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "musiq_eval_dataset = datasets.load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"eval\": eval_dir + \"musiq.json\"},\n",
    "    split = \"eval\",\n",
    "    field = \"data\"\n",
    ")\n",
    "\n",
    "print(\"Sample Eval -\", musiq_eval_dataset[0])\n",
    "print(\"MUSIQ -\", sum(musiq_eval_dataset[\"Score\"]) / len(musiq_eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bfbf28-9c30-4988-8283-90d0318b4c5b",
   "metadata": {},
   "source": [
    "### PIQE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a41ae8a-127f-4da3-a4d5-ba8b22456fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "piqe = pyiqa.create_metric(\"piqe\", device = device)\n",
    "\n",
    "result = {\"data\": []}\n",
    "for prompt in tqdm(prompts):\n",
    "    images = [Image.open(image_gen_dir + f\"{prompt.split(' ')[-1]}_{i}.png\") for i in range(images_evaluated_per_prompt)]\n",
    "    images = torch.tensor(np.stack(images)).permute(0, 3, 1, 2).to(device) / 255\n",
    "    piqe_score = piqe(images).detach().cpu().mean().item()\n",
    "    result[\"data\"].append({\"Prompt\": prompt, \"Score\": piqe_score})\n",
    "    \n",
    "with open(eval_dir + \"piqe.json\", \"w\") as f:\n",
    "    f.write(json.dumps(result))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06908fee-7ead-4b3d-a9f2-f5b467bf92f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "piqe_eval_dataset = datasets.load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"eval\": eval_dir + \"piqe.json\"},\n",
    "    split = \"eval\",\n",
    "    field = \"data\"\n",
    ")\n",
    "\n",
    "print(\"Sample Eval -\", piqe_eval_dataset[0])\n",
    "print(\"PIQE -\", sum(piqe_eval_dataset[\"Score\"]) / len(piqe_eval_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
