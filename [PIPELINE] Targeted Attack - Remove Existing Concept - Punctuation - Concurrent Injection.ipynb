{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad48236c-a3ee-4f42-b0e3-c75d98312a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import optuna\n",
    "import torchvision\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ade51f-2bcc-4ae6-a776-0f05f269c696",
   "metadata": {},
   "source": [
    "## T2I Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba5513-bbfe-4284-ac73-afef2c437095",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    safety_checker = None\n",
    ").to(device)\n",
    "pipeline.set_progress_bar_config(disable = True)\n",
    "\n",
    "def t2i_model(prompt, *, num_images_per_prompt, num_inference_steps, seed):\n",
    "    return pipeline(prompt,\n",
    "                    num_images_per_prompt = num_images_per_prompt,\n",
    "                    num_inference_steps = num_inference_steps,\n",
    "                    generator = torch.manual_seed(seed)).images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e930b-6f48-4829-b0bd-9f11d8a6e1a3",
   "metadata": {},
   "source": [
    "## VLM Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe54e3-3284-4825-ae66-0e7dc192328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from t2v_metrics.t2v_metrics import VQAScore\n",
    "\n",
    "clip_flant5_score = VQAScore(model = \"clip-flant5-xl\")\n",
    "\n",
    "def vlm_evaluator(prompt, adversarial_images):\n",
    "    temp_files = []\n",
    "    temp_file_names = []\n",
    "    for adversarial_image in adversarial_images:\n",
    "        temp_file = tempfile.NamedTemporaryFile(suffix = \".png\")\n",
    "        adversarial_image.save(temp_file.name)\n",
    "        temp_files.append(temp_file)\n",
    "        temp_file_names.append(temp_file.name)\n",
    "\n",
    "    avg_vqa_score = clip_flant5_score(images = temp_file_names, texts = [prompt]).detach().cpu().mean().item()\n",
    "    \n",
    "    for temp_file in temp_files:\n",
    "        temp_file.close()\n",
    "        \n",
    "    return avg_vqa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9cd315-9bfe-4feb-b79a-453a163664b6",
   "metadata": {},
   "source": [
    "## Adversarial Prompt Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f8c16-3a19-41f8-bc66-21b3c84783e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Objective:\n",
    "    def __init__(self, prompt, removed_concept, t2i_model, vlm_evaluator, seed, *, m, k, t):\n",
    "        self.prompt = prompt\n",
    "        self.removed_concept = removed_concept\n",
    "        self.split_prompt = self.prompt.strip().split(\" \")\n",
    "        self.all_pos = len(self.split_prompt) + 1\n",
    "\n",
    "        self.t2i_model = t2i_model\n",
    "        self.vlm_evaluator = vlm_evaluator\n",
    "        \n",
    "        self.seed = seed\n",
    "        self.m = m\n",
    "        self.k = k\n",
    "        self.t = t\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        char_pos_list = [[] for pos in range(self.all_pos)]\n",
    "        for i in range(self.m):\n",
    "            char = trial.suggest_categorical(f\"char_{i}\", string.punctuation)\n",
    "            pos = trial.suggest_int(f\"pos_{i}\", 0, len(self.split_prompt)) # low and high are inclusive\n",
    "            char_pos_list[pos].append(char)\n",
    "\n",
    "        adversarial_prompt = []\n",
    "        for pos in range(self.all_pos + len(self.split_prompt)):\n",
    "            word = \" \".join(char_pos_list[pos // 2]) if pos % 2 == 0 else self.split_prompt[pos // 2]\n",
    "            if word:\n",
    "                adversarial_prompt.append(word)\n",
    "        adversarial_prompt = \" \".join(adversarial_prompt)\n",
    "        \n",
    "        adversarial_images = t2i_model(adversarial_prompt,\n",
    "                                       num_images_per_prompt = self.k,\n",
    "                                       num_inference_steps = self.t,\n",
    "                                       seed = self.seed)\n",
    "        return vlm_evaluator(self.removed_concept, adversarial_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ffae12-eb22-4254-a786-5e5e7435d6f3",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352831fc-34d2-4c5b-b4db-aec0b3ccbc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50 # number of permutations/trials\n",
    "m = 3 # number of punctuations injected\n",
    "k = 3 # number of images generated per permutation\n",
    "t = 1 # number of inference steps\n",
    "sampler = optuna.samplers.NSGAIISampler(seed = seed) # sampler to get approximate best permutation of perturbations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2124251-2db9-44cd-ae76-61d8acf6d3ba",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8bbc76-f3c5-4d1e-9b88-c55e6b73d122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template: https://github.com/openai/CLIP/blob/dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1/data/prompts.md?plain=1#L669\n",
    "concepts = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"] # CIFAR10\n",
    "prompts = [f\"a photo of a {concept}\" for concept in concepts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a446732-2ed9-4181-b57b-8ec45bc3b1f6",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d906b1f3-1908-49c0-8646-17f0a9a9da10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "result = {\"data\": []}\n",
    "for prompt, removed_concept in tqdm(list(zip(prompts, concepts))):\n",
    "    # Optimize Adversarial Prompt\n",
    "    study = optuna.create_study(direction = \"minimize\", sampler = sampler)\n",
    "    objective = Objective(prompt, removed_concept, t2i_model, vlm_evaluator, seed,\n",
    "                          m = m, k = k, t = t)\n",
    "    study.optimize(objective, n_trials = n, show_progress_bar = False)\n",
    "\n",
    "    # Retrieve Adversarial Prompt\n",
    "    split_prompt = prompt.strip().split(\" \")\n",
    "    all_pos = len(split_prompt) + 1\n",
    "    char_pos_list = [[] for pos in range(all_pos)]\n",
    "    adversarial_prompt = []\n",
    "    for i in range(m):\n",
    "        char_pos_list[study.best_params[f\"pos_{i}\"]].append(study.best_params[f\"char_{i}\"])\n",
    "    for pos in range(all_pos + len(split_prompt)):\n",
    "        word = \" \".join(char_pos_list[pos // 2]) if pos % 2 == 0 else split_prompt[pos // 2]\n",
    "        if word:\n",
    "            adversarial_prompt.append(word)\n",
    "    adversarial_prompt = \" \".join(adversarial_prompt)\n",
    "\n",
    "    # Record Best Adversarial Prompt\n",
    "    result[\"data\"].append({\"Original Prompt\": prompt, \"Removed Concept\": removed_concept, \"Adversarial Prompt\": adversarial_prompt, \"Approx. VQAScore\": study.best_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0da1513-7904-4870-87a2-2fe3b4875a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in result[\"data\"]:\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
