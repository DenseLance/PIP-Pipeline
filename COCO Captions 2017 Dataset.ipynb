{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0031cc13-5ba0-4260-856e-8a4cd9e045c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "import datasets\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"MODELSCOPE_LOG_LEVEL\"] = str(logging.ERROR)\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ce176-8bc8-472f-a1ed-87279e795731",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3790330-5fe6-4908-a49e-7890b20e53f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dataset = datasets.load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"train\": \"captions_train2017.json\", \"validation\": \"captions_val2017.json\"},\n",
    "    split = \"validation\",\n",
    "    field = \"images\"\n",
    ")\n",
    "\n",
    "captions_dataset = datasets.load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"train\": \"captions_train2017.json\", \"validation\": \"captions_val2017.json\"},\n",
    "    split = \"validation\",\n",
    "    field = \"annotations\"\n",
    ")\n",
    "\n",
    "# Print all images that have more/less than 5 captions in the original dataset\n",
    "##d = {}\n",
    "##for entry in captions_dataset:\n",
    "##    if entry[\"image_id\"] not in d:\n",
    "##        d[entry[\"image_id\"]] = []\n",
    "##    d[entry[\"image_id\"]].append(entry[\"caption\"])\n",
    "##for item in d:\n",
    "##    if len(d[item]) != 5:\n",
    "##        print(\"Image ID:\", item, \"- Number of Captions:\", len(d[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4840e3c-6bed-48ea-832c-91ebcafa37a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prune dataset so that each image has one caption and each caption has one image\n",
    "##pruned_captions_dataset = pd.DataFrame(captions_dataset)\n",
    "##pruned_captions_dataset = pruned_captions_dataset.drop_duplicates(subset = \"caption\", keep = \"first\", ignore_index = True)\n",
    "##pruned_captions_dataset = pruned_captions_dataset.drop_duplicates(subset = \"image_id\", keep = \"first\", ignore_index = True)\n",
    "##pruned_captions_dataset.to_json(\"pruned_captions_val2017.json\", orient = \"records\")\n",
    "\n",
    "# Prove bijection for pruned dataset\n",
    "##assert pruned_captions_dataset[\"caption\"].nunique() == pruned_captions_dataset[\"image_id\"].nunique() == len(pruned_captions_dataset)\n",
    "\n",
    "# Generate new dataset by taking only the first 4% of those captions to form 200 prompts\n",
    "captions_dataset = datasets.load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"validation\": \"pruned_captions_val2017.json\"},\n",
    "    split = \"validation[:4%]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535188a8-7e6a-4bdf-adb0-1f9ca070736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = 0\n",
    "for caption in captions_dataset[\"caption\"]:\n",
    "    word_count += len(caption.strip().split(\" \"))\n",
    "\n",
    "print(\"Average Word Count Per Prompt:\", word_count / len(captions_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf30646-1174-4b37-b8e4-5fc85fe76397",
   "metadata": {},
   "source": [
    "### Download Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2584866-1c35-43d9-af40-8ac4ff40baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import urllib\n",
    "\n",
    "for image in tqdm(images_dataset, desc = \"Downloading images\"):\n",
    "    urllib.request.urlretrieve(image[\"coco_url\"], f\"coco_images/{image['id']}.jpg\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600260c5-0879-4830-a25b-8ce61555bcea",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "* <b>Alignment:</b> CLIPScore, DSG, VQAScore\n",
    "* <b>Quality:</b> FID\n",
    "* <b>Attack Perceptibility:</b> L-Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379c41a7-aa2b-45e4-86cc-cfa3c752fca4",
   "metadata": {},
   "source": [
    "### DSG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda257d-f7a9-4d55-adae-1c301043423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DSG.dsg.query_utils import generate_dsg\n",
    "from DSG.dsg.vqa_utils import MPLUG, calc_vqa_score\n",
    "from DSG.dsg.parse_utils import parse_question_output\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "vqa_model = MPLUG()\n",
    "vqa_model.pipeline_vqa.use_reentrant = False\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B\", device_map = device, torch_dtype = torch.bfloat16)\n",
    "llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm.generation_config.pad_token_id = llm_tokenizer.pad_token_id\n",
    "\n",
    "def autocomplete(prompt, max_new_tokens = 256, **kwargs):\n",
    "    inputs = llm_tokenizer([prompt], return_tensors = \"pt\", padding = True).to(device)\n",
    "    output_ids = llm.generate(**inputs, generation_config = llm.generation_config, max_new_tokens = max_new_tokens, **kwargs)\n",
    "    return llm_tokenizer.batch_decode(output_ids[:, inputs.input_ids.size(dim = 1):])[0].rstrip(llm_tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e884a0e-0a10-492c-8a66-15d9850a9894",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2prompts = {i: {\"input\": caption} for i, caption in enumerate(captions_dataset[\"caption\"])}\n",
    "\n",
    "_, id2question_outputs, _ = generate_dsg(id2prompts, generate_fn = autocomplete, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d528ffd1-5d88-4c1f-98c6-7fc1a3519fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\"data\": []}\n",
    "for i in tqdm(id2prompts):\n",
    "    image = Image.open(f\"coco_images/{captions_dataset[i]['image_id']}.jpg\")\n",
    "    qid2question = parse_question_output(id2question_outputs[i][\"output\"])\n",
    "    qid2answer = {qid: vqa_model.vqa(image, question).lower() for qid, question in qid2question.items()}\n",
    "    result[\"data\"].append({\"Prompt\": captions_dataset[i][\"caption\"], \"VQA\": {\"Question\": qid2question, \"Answer\": qid2answer}, \"Score\": calc_vqa_score(qid2answer)[\"average_score_without_dependency\"]})\n",
    "\n",
    "with open(\"eval/coco_captions_dsg.json\", \"w\") as f:\n",
    "    f.write(json.dumps(result))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251de84f-1572-47e7-a00c-4b6d45598bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsg_eval_dataset = datasets.load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"eval\": \"eval/coco_captions_dsg.json\"},\n",
    "    split = \"eval\",\n",
    "    field = \"data\"\n",
    ")\n",
    "\n",
    "print(\"Sample Eval -\", dsg_eval_dataset[0])\n",
    "print(\"DSG -\", sum(dsg_eval_dataset[\"Score\"]) / len(dsg_eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e0badd-20f7-4390-b9d1-6b424a09a300",
   "metadata": {},
   "source": [
    "### VQAScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e042cf-c1f4-49d0-8980-ddce984c29dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from t2v_metrics.t2v_metrics import VQAScore\n",
    "\n",
    "clip_flant5_score = VQAScore(model = \"clip-flant5-xl\")\n",
    "\n",
    "result = {\"data\": []}\n",
    "for entry in tqdm(captions_dataset):\n",
    "    result[\"data\"].append({\"Prompt\": entry[\"caption\"], \"Score\": clip_flant5_score(images = [f\"coco_images/{entry['image_id']}.jpg\"], texts = [entry[\"caption\"]]).detach().cpu().item()})\n",
    "\n",
    "with open(\"eval/coco_captions_vqascore.json\", \"w\") as f:\n",
    "    f.write(json.dumps(result))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b03eff-3e8e-40d1-9298-e9ac2b668d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqascore_eval_dataset = datasets.load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"eval\": \"eval/coco_captions_vqascore.json\"},\n",
    "    split = \"eval\",\n",
    "    field = \"data\"\n",
    ")\n",
    "\n",
    "print(\"Sample Eval -\", vqascore_eval_dataset[0])\n",
    "print(\"VQAScore -\", sum(vqascore_eval_dataset[\"Score\"]) / len(vqascore_eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396755a8-e960-4f20-a88e-6df089d78248",
   "metadata": {},
   "source": [
    "### CLIPScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca83556-8e97-4f4f-a575-5498d9df7e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchmetrics.multimodal.clip_score import CLIPScore\n",
    "\n",
    "clip_score = CLIPScore(model_name_or_path = \"openai/clip-vit-large-patch14\").to(device)\n",
    "\n",
    "result = {\"data\": []}\n",
    "for entry in tqdm(captions_dataset):\n",
    "    image = Image.open(f\"coco_images/{entry['image_id']}.jpg\").convert(\"RGB\")\n",
    "    result[\"data\"].append({\"Prompt\": entry[\"caption\"], \"Score\": clip_score(torch.tensor(np.asarray(image)).permute(2, 0, 1).to(device), entry[\"caption\"]).detach().cpu().item()})\n",
    "    \n",
    "with open(\"eval/coco_captions_clipscore.json\", \"w\") as f:\n",
    "    f.write(json.dumps(result))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804127c9-c1db-4bae-ba9b-22f83f4d6552",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_eval_dataset = datasets.load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"eval\": \"eval/coco_captions_clipscore.json\"},\n",
    "    split = \"eval\",\n",
    "    field = \"data\"\n",
    ")\n",
    "\n",
    "print(\"Sample Eval -\", clip_eval_dataset[0])\n",
    "print(\"CLIPScore -\", sum(clip_eval_dataset[\"Score\"]) / len(clip_eval_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
