{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0031cc13-5ba0-4260-856e-8a4cd9e045c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pyiqa\n",
    "import logging\n",
    "import datasets\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"MODELSCOPE_LOG_LEVEL\"] = str(logging.ERROR)\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "seed = 42\n",
    "images_evaluated_per_prompt = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ce176-8bc8-472f-a1ed-87279e795731",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3790330-5fe6-4908-a49e-7890b20e53f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 250 # number of permutations/trials\n",
    "m = 3 # number of punctuations injected\n",
    "k = 3 # number of images generated per permutation\n",
    "t = 20 # number of inference steps\n",
    "sampler_name = \"NSGAIISampler\" # sampler to get approximate best permutation of perturbations\n",
    "original_prompt_dir = \"generated_images/coco/original_prompt/\"\n",
    "result_dir = f\"coco/untargeted_attack/punctuation/concurrent_injection/n={n}_m={m}_k={k}_t={t}_{sampler_name}/\"\n",
    "image_gen_dir = \"generated_images/\" + result_dir\n",
    "eval_dir = \"eval/\" + result_dir\n",
    "\n",
    "adversarial_prompts_dataset = datasets.load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"eval\": eval_dir + \"results.json\"},\n",
    "    split = \"eval\",\n",
    "    field = \"data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05933ad9-dbe4-4130-a919-f6ad534eeae0",
   "metadata": {},
   "source": [
    "## Generate Images for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea8831e-7729-423e-8040-e9b5a2130b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    safety_checker = None\n",
    ").to(device)\n",
    "pipeline.set_progress_bar_config(disable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a4c78b-cbf8-4e26-b4fa-d5ad356a02c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(image_gen_dir, exist_ok = True)\n",
    "for entry in tqdm(adversarial_prompts_dataset):\n",
    "    adversarial_images = pipeline(entry[\"Adversarial Prompt\"].strip(), num_images_per_prompt = images_evaluated_per_prompt, generator = torch.manual_seed(seed)).images\n",
    "    for i, adversarial_image in enumerate(adversarial_images):\n",
    "        adversarial_image.save(image_gen_dir + f\"{entry['id']}_{i}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600260c5-0879-4830-a25b-8ce61555bcea",
   "metadata": {},
   "source": [
    "## Alignment Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379c41a7-aa2b-45e4-86cc-cfa3c752fca4",
   "metadata": {},
   "source": [
    "### DSG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97ba083-a9ab-484b-9b29-b02e3f341aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DSG.dsg.vqa_utils import MPLUG, calc_vqa_score\n",
    "\n",
    "vqa_model = MPLUG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd4fa2-6eb3-4e78-aa83-c26255c422fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eval/coco_captions_dsg.json\", \"r\") as f:\n",
    "    dsg_eval_dataset = json.load(f)\n",
    "    questions = dsg_eval_dataset[\"data\"]\n",
    "    f.close()\n",
    "\n",
    "result = {\"data\": []}\n",
    "for index, entry in enumerate(tqdm(adversarial_prompts_dataset)):\n",
    "    adversarial_images = [Image.open(image_gen_dir + f\"{entry['id']}_{i}.png\") for i in range(images_evaluated_per_prompt)]\n",
    "    qid2question = questions[index][\"VQA\"][\"Question\"]\n",
    "    qid2answers = [{qid: vqa_model.vqa(adversarial_image, question).lower() for qid, question in qid2question.items()} for adversarial_image in adversarial_images]\n",
    "    dsg_scores = [calc_vqa_score(qid2answer)[\"average_score_without_dependency\"] for qid2answer in qid2answers]\n",
    "    result[\"data\"].append({\"Adversarial Prompt\": entry[\"Adversarial Prompt\"], \"VQA\": {\"Question\": qid2question, **{f\"Answer {i}\": qid2answer for i, qid2answer in enumerate(qid2answers)}}, \"Score\": sum(dsg_scores) / len(dsg_scores)})\n",
    "\n",
    "with open(eval_dir + \"dsg.json\", \"w\") as f:\n",
    "    f.write(json.dumps(result))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9afcfa-105e-4168-9c7d-683b7df8c8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsg_eval_dataset = datasets.load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"eval\": eval_dir + \"dsg.json\"},\n",
    "    split = \"eval\",\n",
    "    field = \"data\"\n",
    ")\n",
    "\n",
    "print(\"Sample Eval -\", dsg_eval_dataset[0])\n",
    "print(\"DSG -\", sum(dsg_eval_dataset[\"Score\"]) / len(dsg_eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e0badd-20f7-4390-b9d1-6b424a09a300",
   "metadata": {},
   "source": [
    "### VQAScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e042cf-c1f4-49d0-8980-ddce984c29dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from t2v_metrics.t2v_metrics import VQAScore\n",
    "\n",
    "clip_flant5_score = VQAScore(model = \"clip-flant5-xl\")\n",
    "\n",
    "result = {\"data\": []}\n",
    "for entry in tqdm(adversarial_prompts_dataset):\n",
    "    result[\"data\"].append({\"Adversarial Prompt\": entry[\"Adversarial Prompt\"], \"Score\": clip_flant5_score(images = [image_gen_dir + f\"{entry['id']}_{i}.png\" for i in range(images_evaluated_per_prompt)], texts = [entry[\"Original Prompt\"]]).detach().cpu().mean().item()})\n",
    "\n",
    "with open(eval_dir + \"vqascore.json\", \"w\") as f:\n",
    "    f.write(json.dumps(result))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b03eff-3e8e-40d1-9298-e9ac2b668d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqascore_eval_dataset = datasets.load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"eval\": eval_dir + \"vqascore.json\"},\n",
    "    split = \"eval\",\n",
    "    field = \"data\"\n",
    ")\n",
    "\n",
    "print(\"Sample Eval -\", vqascore_eval_dataset[0])\n",
    "print(\"VQAScore -\", sum(vqascore_eval_dataset[\"Score\"]) / len(vqascore_eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396755a8-e960-4f20-a88e-6df089d78248",
   "metadata": {},
   "source": [
    "### CLIPScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca83556-8e97-4f4f-a575-5498d9df7e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.multimodal.clip_score import CLIPScore\n",
    "\n",
    "clip_score = CLIPScore(model_name_or_path = \"openai/clip-vit-large-patch14\").to(device)\n",
    "\n",
    "result = {\"data\": []}\n",
    "for entry in tqdm(adversarial_prompts_dataset):\n",
    "    adversarial_images = [Image.open(image_gen_dir + f\"{entry['id']}_{i}.png\") for i in range(images_evaluated_per_prompt)]\n",
    "    clip_scores = [clip_score(torch.tensor(np.asarray(adversarial_image)).permute(2, 0, 1).to(device), entry[\"Original Prompt\"]).detach().cpu().item() for adversarial_image in adversarial_images]\n",
    "    result[\"data\"].append({\"Adversarial Prompt\": entry[\"Adversarial Prompt\"], \"Score\": sum(clip_scores) / len(clip_scores)})\n",
    "    \n",
    "with open(eval_dir + \"clipscore.json\", \"w\") as f:\n",
    "    f.write(json.dumps(result))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804127c9-c1db-4bae-ba9b-22f83f4d6552",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_eval_dataset = datasets.load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"eval\": eval_dir + \"clipscore.json\"},\n",
    "    split = \"eval\",\n",
    "    field = \"data\"\n",
    ")\n",
    "\n",
    "print(\"Sample Eval -\", clip_eval_dataset[0])\n",
    "print(\"CLIPScore -\", sum(clip_eval_dataset[\"Score\"]) / len(clip_eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58fd2a3-3a33-4f18-a3c9-ee0b91491092",
   "metadata": {},
   "source": [
    "### FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0d0bfc-7485-45b1-bfcf-c0bbeffd6197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "fid = FrechetInceptionDistance(feature = 192).to(device)\n",
    "images = [Image.open(original_prompt_dir + f\"{entry['id']}_{i}.png\") for i in range(images_evaluated_per_prompt) for entry in adversarial_prompts_dataset]\n",
    "images = [torch.tensor(np.asarray(image)).permute(2, 0, 1).to(device) for image in images]\n",
    "adversarial_images = [Image.open(image_gen_dir + f\"{entry['id']}_{i}.png\") for i in range(images_evaluated_per_prompt) for entry in adversarial_prompts_dataset]\n",
    "adversarial_images = [torch.tensor(np.asarray(adversarial_image)).permute(2, 0, 1).to(device) for adversarial_image in adversarial_images]\n",
    "fid.update(torch.stack(images), real = True)\n",
    "fid.update(torch.stack(adversarial_images), real = False)\n",
    "print(\"FID -\", fid.compute().detach().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6633153-0e55-46ea-86c2-beac641d9ec5",
   "metadata": {},
   "source": [
    "## Quality Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d43b44-f32a-4dbe-b810-04d9b3150053",
   "metadata": {},
   "source": [
    "### LIQE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3cb714-b11a-4a10-a855-47d222b11161",
   "metadata": {},
   "outputs": [],
   "source": [
    "liqe = pyiqa.create_metric(\"liqe\", device = device)\n",
    "\n",
    "result = {\"data\": []}\n",
    "for entry in tqdm(adversarial_prompts_dataset):\n",
    "    adversarial_images = [Image.open(image_gen_dir + f\"{entry['id']}_{i}.png\") for i in range(images_evaluated_per_prompt)]\n",
    "    adversarial_images = torch.tensor(np.stack(adversarial_images)).permute(0, 3, 1, 2).to(device) / 255\n",
    "    liqe_score = liqe(adversarial_images).detach().cpu().mean().item()\n",
    "    result[\"data\"].append({\"Adversarial Prompt\": entry[\"Adversarial Prompt\"], \"Score\": liqe_score})\n",
    "    \n",
    "with open(eval_dir + \"liqe.json\", \"w\") as f:\n",
    "    f.write(json.dumps(result))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76a3325-2b83-4e40-b297-075d896e0c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "liqe_eval_dataset = datasets.load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"eval\": eval_dir + \"liqe.json\"},\n",
    "    split = \"eval\",\n",
    "    field = \"data\"\n",
    ")\n",
    "\n",
    "print(\"Sample Eval -\", liqe_eval_dataset[0])\n",
    "print(\"LIQE -\", sum(liqe_eval_dataset[\"Score\"]) / len(liqe_eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2efa776-0c9e-4ae8-983b-0f06bf759b8a",
   "metadata": {},
   "source": [
    "### MUSIQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f443a0f-88ee-494a-9e61-b8dcced571de",
   "metadata": {},
   "outputs": [],
   "source": [
    "musiq = pyiqa.create_metric(\"musiq\", device = device)\n",
    "\n",
    "result = {\"data\": []}\n",
    "for entry in tqdm(adversarial_prompts_dataset):\n",
    "    adversarial_images = [Image.open(image_gen_dir + f\"{entry['id']}_{i}.png\") for i in range(images_evaluated_per_prompt)]\n",
    "    adversarial_images = torch.tensor(np.stack(adversarial_images)).permute(0, 3, 1, 2).to(device) / 255\n",
    "    musiq_score = musiq(adversarial_images).detach().cpu().mean().item()\n",
    "    result[\"data\"].append({\"Adversarial Prompt\": entry[\"Adversarial Prompt\"], \"Score\": musiq_score})\n",
    "    \n",
    "with open(eval_dir + \"musiq.json\", \"w\") as f:\n",
    "    f.write(json.dumps(result))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3032052-5e14-46bf-b864-e6ac02525ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "musiq_eval_dataset = datasets.load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"eval\": eval_dir + \"musiq.json\"},\n",
    "    split = \"eval\",\n",
    "    field = \"data\"\n",
    ")\n",
    "\n",
    "print(\"Sample Eval -\", musiq_eval_dataset[0])\n",
    "print(\"MUSIQ -\", sum(musiq_eval_dataset[\"Score\"]) / len(musiq_eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77392a8-ac58-49d6-b873-232b5ebd0e2b",
   "metadata": {},
   "source": [
    "### PIQE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d7edc1-5703-453f-af71-eb26cf5cfa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "piqe = pyiqa.create_metric(\"piqe\", device = device)\n",
    "\n",
    "result = {\"data\": []}\n",
    "for entry in tqdm(adversarial_prompts_dataset):\n",
    "    adversarial_images = [Image.open(image_gen_dir + f\"{entry['id']}_{i}.png\") for i in range(images_evaluated_per_prompt)]\n",
    "    adversarial_images = torch.tensor(np.stack(adversarial_images)).permute(0, 3, 1, 2).to(device) / 255\n",
    "    piqe_score = piqe(adversarial_images).detach().cpu().mean().item()\n",
    "    result[\"data\"].append({\"Adversarial Prompt\": entry[\"Adversarial Prompt\"], \"Score\": piqe_score})\n",
    "    \n",
    "with open(eval_dir + \"piqe.json\", \"w\") as f:\n",
    "    f.write(json.dumps(result))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6971c7-8d10-493a-b690-8b24bb819e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "piqe_eval_dataset = datasets.load_dataset(\n",
    "    \"json\",\n",
    "    data_files = {\"eval\": eval_dir + \"piqe.json\"},\n",
    "    split = \"eval\",\n",
    "    field = \"data\"\n",
    ")\n",
    "\n",
    "print(\"Sample Eval -\", piqe_eval_dataset[0])\n",
    "print(\"PIQE -\", sum(piqe_eval_dataset[\"Score\"]) / len(piqe_eval_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
